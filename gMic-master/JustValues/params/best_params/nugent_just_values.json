{"activation": "relu", "batch_size": 10.0, "dropout": 0.3589474309552942, "epochs": 200.0, "layer_1": 69.0, "layer_2": 27.0, "learning_rate": 0.0587304648541852, "optimizer": "adam", "preweight": 2.0, "regularization": 0.0001713915863346, "test_frac": 0.15, "train_frac": 0.7}